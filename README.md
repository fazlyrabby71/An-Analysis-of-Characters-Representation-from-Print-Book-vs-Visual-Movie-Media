# OurGoal

Our goal is Given an input video to select a  bunch of frames to create a summary video that can capture the vital information of the input video. With many books getting adapted as movies, video summarization provides a useful tool that assists video analysis. In this paper, we formulate a video movie summarization to determine the importance of any character in movies with their book counterparts. We propose a fully convolutional sequence network model classifier to solve the video summarization problem, a search model to analyze book representation and statistical analysis for comparison.

Review-

1.Screen time of an actor in a movie or an episode is very important. Many actors get paid according to their total screen time. Moreover, we also want to know how much time our favorite character acted on screen.With the advancement of deep learning now its possible to solve various difficult problems.To solve any problem with deep learning, the first requirement is the data.[https://theailearner.com/2019/03/24/calculating-screen-time-of-an-actor-using-deep-learning/]

2.More than 80% of internet traffic will be real data except in particular, it could be video data[3]. It gives us a certain intuitive sense of how much video data there and how many tools to manage the massive amount of video data. Video recorded in one day could have 5 minutes of video summarization instead of 24 hours video. We can use video summarization for thumbnail generation, if we put a mouse around the video on youtube, we will see that the thumbnail option for the summary of the video, which includes the main video is for. A fair bit of the summarization system can automatically generate those thumbnails. In video Customization, if we put a different video on social media on another platform, they show they have a limitation on how long that video can be. Somehow we need to adjust those accordingly. We can use Video Summarization for a better summary In highlight creation compare it with our text analyzer Before we watch any movie based on our favorite book.[https://openaccess.thecvf.com/content_ECCV_2018/papers/Mrigank_Rochan_Video_Summarization_Using_ECCV_2018_paper.pdf]

3.The idea is nothing but videos are nothing but a collection of a set of images. These images are called frames and can be combined to get the original video. So, a problem related to video data is not that different from an image classification or an object detection problem [https://www.analyticsvidhya.com/blog/2018/09/deep-learning-video-classification-python/]

4.In most simplistic terms. BERT stands for Bidirectional Encoder Representation from Transformers. It is a language representation model and basically a trained transformers encoder stack. BERT applies bidirectional training of transformer encoders using Auto-encoder language modeling. It performs two unsupervised learning tasks during pertaining, one is masked language modeling and the other one is next sentence prediction tasks. It finally achieves state-of-the-art results on GLUE, MNLI, SQuAD data-sets. If we look at the architecture of BERT. It consists of 12 stacked encoder units in the base model and 24 stacked encoder units in the large model. Each encoder unit has a similar architecture to the Transformer model which is an attention-based model having encoder-decoder architecture. Coming to the architecture of encoders it consists of two layers one is a multi-headed attention layer and another one is a position-wise fully connected feed-forward neural network. The attention layer calculates attention on the input embedding using this formula which is then added, normalized, and passed to the neural network.

5. Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a necessary component of machine translation systems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73% F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP researchers and practitioners. In this paper, the writers describe thier BERT-based approach to solving the problem of gender-balanced pronoun resolution. They are able to reach 92% F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team. [https://arxiv.org/pdf/1906.01161.pdf]#

6. Named Entity Recognition (NER) is a key component in NLP systems for question answering, information retrieval, relation extraction, etc. NER systems have been studied and developed widely for decades, but accurate systems using deep neural networks (NN) have only been introduced in the last few years. We present a comprehensive survey of deep neural network architectures for NER, and contrast them with previous approaches to NER based on feature engineering and other supervised or semi-supervised learning algorithms. Our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature-based NER systems can yield further improvements. [https://www.aclweb.org/anthology/C18-1182.pdf]#

7. Named entity recognition (NER) is frequently addressed as a sequence classification task with each input consisting of one sentence of text. It is nevertheless clear that useful information for NER is often found also elsewhere in text. Recent self-attention models like BERT can both capture long-distance relationships in input and represent inputs consisting of several sentences. This creates opportunities for adding cross-sentence information in natural language processing tasks. This paper presents a systematic study exploring the use of cross-sentence information for NER using BERT models in five languages. The writers find that adding context as additional sentences to BERT input systematically increases NER performance. Multiple sentences in input samples allows us to study the predictions of the sentences in different contexts. They propose a straightforward method, Contextual Majority Voting (CMV), to combine these different predictions and demonstrate this to further increase NER performance. Evaluation on established datasets, including the CoNLL’02 and CoNLL’03 NER benchmarks, demonstrates that their proposed approach can improve on the state-of-the-art NER results on English, Dutch, and Finnish, achieves the best reported BERT-based results on German, and is on par with other BERT-based approaches in Spanish.[https://www.aclweb.org/anthology/2020.coling-main.78.pdf]#

8. In many NLP applications like search and information extraction for named entities, it is necessary to find all the mentions of a named entity, some of which appear as pronouns (she, his, etc.) or nominals (the professor, the German chancellor, etc.). It is therefore important that coreference resolution systems are able to link these different types of mentions to the correct entity name. We evaluate state-of-theart coreference resolution systems for the task of resolving all mentions to named entities. The analysis reveals that standard coreference metrics do not reflect adequately the requirements in this task: they do not penalize systems for not identifying any mentions by name to an entity and they reward systems even if systems find correctly mentions to the same entity but fail to link these to a proper name (she–the student–no name). We introduce new metrics for evaluating named entity coreference that address these discrepancies and show that for the comparisons of competitive systems, standard coreference evaluations could give misleading results for this task. We are, however, able to confirm that the state-of-the art system according to traditional evaluations also performs vastly better than other systems on the named entity coreference task. [https://www.aclweb.org/anthology/W19-2801.pdf]#

9. Pronoun resolution is a major area of natural language understanding. However, large-scale training sets are still scarce, since manually labelling data is costly. In this work, the researchers introduce WIKICREM (Wikipedia CoREferences Masked) a large-scale, yet accurate dataset of pronoun disambiguation instances. They use a language-model-based approach for pronoun resolution in combination with their WIKICREM dataset. They compare a series of models on a collection of diverse and challenging coreference resolution problems, where they match or outperform previous state-of-theart approaches on 6 out of 7 datasets, such as GAP, DPR, WNLI, PDP, WINOBIAS, and WINOGENDER. They release their model to be used off-the-shelf for solving pronoun disambiguation. [https://www.aclweb.org/anthology/D19-1439.pdf]#

10. Grounding a pronoun to a visual object it refers to requires complex reasoning from various information sources, especially in conversational scenarios. For example, when people in a conversation talk about something all speakers can see, they often directly use pronouns (e.g., it) to refer to it without previous introduction. This fact brings a huge challenge for modern natural language understanding systems, particularly conventional contextbased pronoun coreference models. To tackle this challenge, in this paper, the researchers formally define the task of visual-aware pronoun coreference resolution (PCR) and introduce VisPro, a large-scale dialogue PCR dataset, to investigate whether and how the visual information can help resolve pronouns in dialogues. They then propose a novel visual-aware PCR model, VisCoref, for this task and conduct comprehensive experiments and case studies on their dataset. Results demonstrate the importance of the visual information in this PCR case and show the effectiveness of the proposed model. [https://www.aclweb.org/anthology/D19-1516.pdf]#

11.Spacy has its own deep learning library called think used under the hood for different NLP models. For most tasks, spaCy uses a deep neural network based on CNN with a few tweaks. Specifically for Named Entity Recognition, spaCy uses.[https://spacy.io/api/doc/]

12.The Python Imaging Library or PIL allowed you to do image processing in Python. The Python Imaging Library adds image processing capabilities to your Python interpreter.This library provides extensive file format support, an efficient internal representation, and fairly powerful image processing capabilities.The core image library is designed for fast access to data stored in a few basic pixel formats. It should provide a solid foundation for a general image processing tool[https://pillow.readthedocs.io/en/stable/]

13.OpenCV is capable of image analysis and processing. This means it is great at taking frames out of video or taking in two frames from a stereoscopic camera and running algorithms to extract information. For example using OpenCV would give us the mathematical tools required to capture images and track a particular object as it moves around. This is not provided directly but the mathematical tools required to process the images to extract such information is available. We can see from the above example that although it can do other things such as stretch an image or change color, it's purpose is not to serve as an image processing engine similar to Photoshop or such. It is intended to be very fast almost real time if the hardware supports it and perform all sorts of functions such as Fourier transforms very fast and then allow us to either glean information or transform the stream of images as we like. So the answer really is that its application is infinite in the domain of video/image analysis and processing including things such as facial recognition, tracking objects, determining distance of objects from a stereoscopic input etc. None of these are given ready made, but the mathematical tools are provided which we need to know how to apply.[https://docs.opencv.org/master/]



